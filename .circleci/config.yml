defaults: &defaults
  docker:
    - image: ubuntu:16.04
  working_directory: ~/kubeadm-dind-cluster

env: &env
  DOCKER_VERSION: "17.03.0-ce"
  KDC_VERSIONS: "1.8 1.9 1.10 1.11"
  IMAGE_NAME: "mirantis/kubeadm-dind-cluster"
  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/root/.kubeadm-dind-cluster
  # prevent dind-cluster*.sh from trying to pull the image
  # during the tests
  DIND_SKIP_PULL: y
  KUBE_RSYNC_PORT: 8730
  # which images should be pushed
  IMAGE_PUSH_FILTER: "mirantis/kubeadm-dind-cluster:v*"
  STABLE_BRANCH: stable
  STABLE_SRC_TAG: "v1.10"
  PRIMARY_BRANCH: master

setup: &setup
  name: Set up the environment
  command: |
    apt-get -qq update
    apt-get install -y curl ca-certificates git liblz4-tool rsync socat tzdata
    curl -sSL -o "/tmp/docker-${DOCKER_VERSION}.tgz" "https://get.docker.com/builds/Linux/x86_64/docker-${DOCKER_VERSION}.tgz"
    tar -xz -C /tmp -f "/tmp/docker-${DOCKER_VERSION}.tgz"
    mv /tmp/docker/* /usr/bin
    # Start port forwarder
    "${PWD}/build/portforward.sh" start

display_cluster_state: &display_cluster_state
  command: |
    DIND_ROOT="$PWD"

    kubectl="kubectl"
    if [[ ${K8S_SRC:-} ]]; then
      cd kubernetes
      kubectl="cluster/kubectl.sh"
    fi

    apiserver_port="$( "${DIND_ROOT}/dind-cluster.sh" apiserver-port )"
    "${DIND_ROOT}/build/portforward.sh" -wait "$apiserver_port"

    "${kubectl}" "--server=:${apiserver_port}" version
    "${kubectl}" "--server=:${apiserver_port}" get all --all-namespaces -o wide
    "${kubectl}" "--server=:${apiserver_port}" get nodes

dump_cluster: &dump_cluster
  command: |
    mkdir -p /tmp/cluster_state
    out="/tmp/cluster_state/dump-1.gz"
    if [[ -f ${out} ]]; then
      out="/tmp/cluster_state/dump-2.gz"
    fi
    if [[ ${K8S_SRC:-} ]]; then
      cd kubernetes
      ../dind-cluster.sh dump | gzip >"${out}"
    else
      ./dind-cluster.sh dump | gzip >"${out}"
    fi

test: &test
  steps:
    - checkout
    - setup_remote_docker
    - run:
        <<: *setup
    - attach_workspace:
        at: _save
    - run:
        name: Restore images
        command: |
          docker load -i _save/images.tar
    - run:
        name: Bring up the cluster
        command: |
          export DIND_PORT_FORWARDER_WAIT=1
          export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"
          ./dind-cluster.sh up
    - run:
        name: Display cluster status (1)
        <<: *display_cluster_state
    - run:
        name: Dump cluster state (1)
        when: always
        <<: *dump_cluster
    - run:
        name: Bring up the cluster (again)
        command: |
          export DIND_PORT_FORWARDER_WAIT=1
          export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"
          ./dind-cluster.sh up
    - run:
        name: Display cluster status (2)
        <<: *display_cluster_state
    - run:
        name: Dump cluster state (2)
        when: always
        <<: *dump_cluster
    - store_artifacts:
        path: /tmp/cluster_state
    - run:
        name: Bring down the cluster
        command: |
          ./dind-cluster.sh down
    - run:
        name: Clean the cluster
        command: |
          ./dind-cluster.sh clean

test_src: &test_src
  steps:
    - checkout
    - setup_remote_docker
    - run:
        <<: *setup
    - attach_workspace:
        at: _save
    - run:
        name: Restore images
        command: |
          docker load -i _save/images.tar
    - run:
        name: Check out Kubernetes source
        command: |
          git clone https://github.com/kubernetes/kubernetes.git
          cd kubernetes
          if [[ ${K8S_SRC_VERSION:-} ]]; then
            git checkout "${K8S_SRC_VERSION}"
          fi
          echo
          echo "*** Latest commit:"
          git --no-pager log -1
    - run:
        name: Bring up the cluster
        no_output_timeout: 35m
        command: |
          export DIND_PORT_FORWARDER_WAIT=1
          export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"
          "${PWD}/build/portforward.sh" 8730
          cd kubernetes
          ../dind-cluster.sh up
    - run:
        name: Display cluster status (1)
        <<: *display_cluster_state
    - run:
        name: Dump cluster state (1)
        when: always
        <<: *dump_cluster
    - run:
        name: Bring up the cluster (again)
        command: |
          export DIND_PORT_FORWARDER_WAIT=1
          export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"
          "${PWD}/build/portforward.sh" 8730
          cd kubernetes
          ../dind-cluster.sh up
    - run:
        name: Run some e2e tests
        no_output_timeout: 25m
        command: |
          export DIND_PORT_FORWARDER_WAIT=1
          export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"
          "${PWD}/build/portforward.sh" 8730
          cd kubernetes
          ../dind-cluster.sh e2e "existing RC"
    - run:
        name: Display cluster status (2)
        <<: *display_cluster_state
    - run:
        name: Dump cluster state (2)
        when: always
        <<: *dump_cluster
    - store_artifacts:
        path: /tmp/cluster_state
    - run:
        name: Bring down the cluster
        command: |
          cd kubernetes
          ../dind-cluster.sh down
    - run:
        name: Clean the cluster
        command: |
          cd kubernetes
          ../dind-cluster.sh clean

test_ipv6_only: &test_ipv6_only
  machine: true
  working_directory: ~/kubeadm-dind-cluster
  steps:
    - checkout
    - attach_workspace:
        at: _save
    - run:
        name: Restore images
        command: |
          docker load -i _save/images.tar
    - run:
        name: Run ipv6 only tests
        no_output_timeout: 25m
        command: |
          "${PWD}/test/test-net-connectivity.sh"

test_multiple_clusters: &test_multiple_clusters
  steps:
    - checkout
    - setup_remote_docker
    - run:
        <<: *setup
    - attach_workspace:
        at: _save
    - run:
        name: Restore images
        command: |
          docker load -i _save/images.tar
    - run:
        name: Bring up multiple clusters in parallel
        no_output_timeout: 35m
        command: |
          set -eu
          set -o pipefail

          function test-case-multiple-instances {
            local defaultLabel='mirantis.kubeadm_dind_cluster_runtime'
            local secondClusterID="20"
            local thirdClusterLabel="third"
            local d="./dind-cluster.sh"

            export DIND_PORT_FORWARDER_WAIT=1
            export DIND_PORT_FORWARDER="${PWD}/build/portforward.sh"

            "${d}" up
            CLUSTER_ID="$secondClusterID" "${d}" up
            DIND_LABEL="$thirdClusterLabel" "${d}" up

            # masters
            test "$(countContainersWithExactName "kube-master")" -eq 1 || {
              fail 'Expected exactly one container with name "kube-master" to exist - cluster created with default label'
            }
            test "$(countContainersWithExactName "kube-master-cluster-${secondClusterID}")" -eq 1 || {
              fail "Expected exactly one container with name 'kube-master-cluster-${secondClusterID}' to exist - cluster created with custom ID"
            }
            test "$(countContainersWithExactName "kube-master-${thirdClusterLabel}")" -eq 1 || {
              fail "Expected exactly one container with name 'kube-master-${thirdClusterLabel}' to exist - cluster created with custom label and assigned ID"
            }

            # nodes
            test "$(countContainersWithExactName "kube-node-\\d{1}")" -ge 1 || {
              fail 'Expected at least one container with name "kube-node-<nr>" to exist - cluster created with default label'
            }
            test "$(countContainersWithExactName "kube-node-\\d{1}-cluster-${secondClusterID}")" -ge 1 || {
              fail "Expected at least one container with name 'kube-node-<nr>-cluster-${secondClusterID}' to exist - cluster created with custom ID"
            }
            test "$(countContainersWithExactName "kube-node-\\d{1}-${thirdClusterLabel}")" -ge 1 || {
              fail "Expected at least one container with name 'kube-node-<nr>-${thirdClusterLabel}' to exist - cluster created with custom label and assigned ID"
            }

            # volumes
            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-master$")" -eq 1 || {
              fail 'Expected one volume for the kube master to exist - cluster created with default label'
            }
            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-node-\\d+$")" -ge 1 || {
              fail 'Expected one volume for the kube nodes to exist - cluster created with default label'
            }

            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-master-cluster-${secondClusterID}$")" -eq 1 || {
              fail 'Expected one volume for the kube master to exist - cluster created with custom ID'
            }
            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-node-\\d+-cluster-${secondClusterID}$")" -ge 1 || {
              fail 'Expected one volume for the kube nodes to exist - cluster created with custom ID'
            }

            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-master-${thirdClusterLabel}$")" -eq 1 || {
              fail 'Expected one volume for the kube master to exist - cluster created with custom label'
            }
            test "$(countVolumesWithFilter "name=kubeadm-dind-kube-node-\\d+-${thirdClusterLabel}$")" -ge 1 || {
              fail 'Expected one volume for the kube nodes to exist - cluster created with custom label'
            }

            if usesLinuxKit
            then
              test "$(countVolumesWithFilter "name=kubeadm-dind-sys$")" -eq 1 || {
                fail 'Expected one volume for the sys to exist - cluster created with default label'
              }
              test "$(countVolumesWithFilter "name=kubeadm-dind-sys-cluster-${secondClusterID}$")" -eq 1 || {
                fail 'Expected one volume for the sys to exist - cluster created with custom ID'
              }
              test "$(countVolumesWithFilter "name=kubeadm-dind-sys-${thirdClusterLabel}$")" -eq 1 || {
                fail 'Expected one volume for the sys to exist - cluster created with custom label'
              }
            fi

            # networks
            test "$(countNetworksWithFilter "name=kubeadm-dind-net$")" -eq 1 || {
              fail 'Expected one network to exist - cluster created with default label'
            }
            test "$(countNetworksWithFilter "name=kubeadm-dind-net-cluster-${secondClusterID}$")" -eq 1 || {
              fail 'Expected one network to exist - cluster created with custom ID'
            }
            test "$(countNetworksWithFilter "name=kubeadm-dind-net-${thirdClusterLabel}$")" -eq 1 || {
              fail 'Expected one network to exist - cluster created with custom label'
            }

            # contexts
            hasKubeContext "dind" || {
              fail 'Expected to have context - cluster created with default label'
            }
            hasKubeContext "dind-cluster-${secondClusterID}" || {
              fail 'Expected to have context - cluster created with custom ID'
            }
            hasKubeContext "dind-${thirdClusterLabel}" || {
              fail 'Expected to have context - cluster created with custom label'
            }

            "${d}" clean
            CLUSTER_ID="$secondClusterID" "${d}" clean
            DIND_LABEL="$thirdClusterLabel" "${d}" clean
          }

          function hasKubeContext() {
            kubectl config get-contexts --no-headers \
              | sed 's/^\s*\**\s*//g' \
              | grep -q "^${1}\\s"
          }

          function fail() {
            local msg="$1"
            echo -e "\033[1;31m${msg}\033[0m" >&2
            return 1
          }

          function countNetworksWithFilter() {
            docker network ls -q --filter="$1" | wc -l | xargs
          }

          function countVolumesWithFilter() {
            docker volume ls -q --filter="$1" | wc -l | xargs
          }

          function countContainersWithExactName() {
            countContainersWithFilter "name=${1}$"
          }

          function countContainersWithLabel() {
            countContainersWithFilter "label=${1}"
          }

          function countContainersWithFilter() {
            docker ps -q --filter="${1}" | wc -l | xargs
          }

          function usesLinuxKit() {
            if ! docker info|grep -s '^Operating System: .*Docker for Windows' > /dev/null 2>&1 ; then
              if docker info|grep -s '^Kernel Version: .*-moby$' >/dev/null 2>&1 ||
                  docker info|grep -s '^Kernel Version: .*-linuxkit-' > /dev/null 2>&1 ; then
                return 0
              fi
            fi
            return 1
          }

          test-case-multiple-instances


version: 2
jobs:
  build:
    <<: *defaults
    environment:
      <<: *env
    steps:
    - checkout
    - setup_remote_docker
    - run:
        <<: *setup
    - run:
        name: Check fixed scripts
        command: |
          build/genfixed.sh
          if git status | grep -q "fixed/"; then
            echo >&2 "*** Fixed scripts not up to date, please update with build/genfixed.sh"
            exit 1
          fi
    - run:
        name: Build images
        command: |
          DIND_ROOT="${PWD}"
          . build/funcs.sh
          dind::build-base
          images_to_save=(${BARE_IMAGE_NAME})
          for v in ${KDC_VERSIONS}; do
            version="${v//./_}"
            eval "HYPERKUBE_URL=\${HYPERKUBE_URL_${version}}"
            eval "HYPERKUBE_SHA1=\${HYPERKUBE_SHA1_${version}}"
            kubeadm_version="${version}"
            eval "KUBEADM_URL=\${KUBEADM_URL_${version}}"
            eval "KUBEADM_SHA1=\${KUBEADM_SHA1_${version}}"
            tag="v${v}"
            cur_image="${IMAGE_NAME}:${tag}"
            dind::build-image "${cur_image}"
            images_to_save+=("${cur_image}")
          done
          mkdir -p _save
          docker save "${images_to_save[@]}" >_save/images.tar
    - persist_to_workspace:
        root: _save
        paths:
        - images.tar

  test_multiple_clusters_1.8:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
    <<: *test_multiple_clusters

  test_multiple_clusters_1.9:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
    <<: *test_multiple_clusters

  test_multiple_clusters_1.10:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
    <<: *test_multiple_clusters

  test_multiple_clusters_1.11:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
    <<: *test_multiple_clusters

  test_multiple_clusters_1.11_ipv6:
    <<: *defaults
    environment:
      <<: *env
      IP_MODE: ipv6
      # Ensure no collisions for NAT64 V4 mapping subnet prefix. First cluster will use 10.100,
      # second will use 10.120 (cluster ID 20), and third will use random value from 10.101 to
      # 10.113 (using legacy DIND_LABEL w/o cluster ID, causes random offet from 1..13 to be
      # added to base prefix.
      NAT64_V4_SUBNET_PREFIX: "10.100"
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
    <<: *test_multiple_clusters

  test_1.8:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
    <<: *test

  test_1.8_flannel:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
      CNI_PLUGIN: flannel
    <<: *test

  test_1.8_calico:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
      CNI_PLUGIN: calico
    <<: *test

  test_1.8_calico_kdd:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
      CNI_PLUGIN: calico-kdd
    <<: *test

  test_1.8_weave:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.8
      CNI_PLUGIN: weave
    <<: *test

  test_1.9:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
    <<: *test

  test_1.9_flannel:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
      CNI_PLUGIN: flannel
    <<: *test

  test_1.9_calico:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
      CNI_PLUGIN: calico
    <<: *test

  test_1.9_calico_kdd:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
      CNI_PLUGIN: calico-kdd
    <<: *test

  test_1.9_weave:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.9
      CNI_PLUGIN: weave
    <<: *test

  test_1.10:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
    <<: *test

  test_1.10_flannel:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: flannel
    <<: *test

  test_1.10_calico:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: calico
    <<: *test

  test_1.10_calico_kdd:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: calico-kdd
    <<: *test

  test_1.10_weave:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: weave
    <<: *test

  test_1.10_kube_router:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: kube-router
    <<: *test

  test_1.10_ptp:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.10
      CNI_PLUGIN: ptp
    <<: *test

  test_1.11:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
    <<: *test

  test_1.11_flannel:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: flannel
    <<: *test

  test_1.11_calico:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: calico
    <<: *test

  test_1.11_calico_kdd:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: calico-kdd
    <<: *test

  test_1.11_weave:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: weave
    <<: *test

  test_1.11_kube_router:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: kube-router
    <<: *test

  test_1.11_ptp:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      CNI_PLUGIN: ptp
    <<: *test

  test_1.9_ipv6_only_nat64:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.9'
    <<: *test_ipv6_only

  test_1.10_ipv6_only_nat64:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.10'
    <<: *test_ipv6_only

  test_1.11_ipv6_only_nat64:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.11'
    <<: *test_ipv6_only

  test_1.9_ipv6_only_ext_aaaa:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.9'
      DIND_ALLOW_AAAA_USE: 'true'
    <<: *test_ipv6_only

  test_1.10_ipv6_only_ext_aaaa:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.10'
      DIND_ALLOW_AAAA_USE: 'true'
    <<: *test_ipv6_only

  test_1.11_ipv6_only_ext_aaaa:
    environment:
      <<: *env
      TEST_K8S_VER: 'v1.11'
      DIND_ALLOW_AAAA_USE: 'true'
    <<: *test_ipv6_only

  test_src_release:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      BUILD_KUBEADM: y
      BUILD_HYPERKUBE: y
      K8S_SRC: y
      K8S_SRC_VERSION: release-1.11
    <<: *test_src

  test_src_master:
    <<: *defaults
    environment:
      <<: *env
      DIND_IMAGE: mirantis/kubeadm-dind-cluster:v1.11
      BUILD_KUBEADM: y
      BUILD_HYPERKUBE: y
      K8S_SRC: y
    <<: *test_src

  push:
    <<: *defaults
    environment:
      <<: *env
    steps:
    - checkout
    - setup_remote_docker
    - run:
        <<: *setup
    - attach_workspace:
        at: _save
    - run:
        name: Restore images
        command: |
          docker load -i _save/images.tar
    - run:
        name: Push images
        command: |
          if [[ ! ${DOCKER_USER:-} || ! ${DOCKER_PASS} ]]; then
            echo >&2 "*** WARNING: can't push images for outside forks"
            exit 0
          fi
          if [[ ${CIRCLE_BRANCH:-} != ${STABLE_BRANCH} && ${CIRCLE_BRANCH:-} != ${PRIMARY_BRANCH} ]]; then
            echo >&2 "*** Not pushing images for the branch: ${CIRCLE_BRANCH}"
            exit 0
          fi
          docker login -u "${DOCKER_USER}" -p "${DOCKER_PASS}"
          if [[ ${CIRCLE_BRANCH} = ${STABLE_BRANCH} ]]; then
            image="${IMAGE_NAME}:${STABLE_SRC_TAG}"
            new_name="${IMAGE_NAME}:${STABLE_BRANCH}"
            docker tag "${image}" "${new_name}"
            docker push "${new_name}"
            exit 0
          fi
          docker images \
                 --format '{{ .Repository }}:{{ .Tag }}' \
                 --filter=reference="${IMAGE_PUSH_FILTER}" |
              while read image; do
            echo >&2 "*** Pushing: ${image}"
            if [[ ${RETAG_PREFIX:-} ]]; then
              new_name="${image/:/:${RETAG_PREFIX}}"
              docker tag "${image}" "${new_name}"
              docker push "${new_name}"
            else
              docker push "${image}"
            fi
          done

workflows:
  version: 2
  build-test-push:
    jobs:
    - build
    - test_multiple_clusters_1.8:
        requires:
        - build
    - test_multiple_clusters_1.9:
        requires:
        - build
    - test_multiple_clusters_1.10:
        requires:
        - build
    - test_multiple_clusters_1.11:
        requires:
        - build
    - test_multiple_clusters_1.11_ipv6:
        requires:
        - build
    - test_1.8:
        requires:
        - build
    - test_1.8_flannel:
        requires:
        - build
    - test_1.8_calico:
        requires:
        - build
    - test_1.8_calico_kdd:
        requires:
        - build
    - test_1.8_weave:
        requires:
        - build
    - test_1.9:
        requires:
        - build
    - test_1.9_flannel:
        requires:
        - build
    - test_1.9_calico:
        requires:
        - build
    - test_1.9_calico_kdd:
        requires:
        - build
    - test_1.9_weave:
        requires:
        - build
    - test_1.10:
        requires:
        - build
    - test_1.10_flannel:
        requires:
        - build
    - test_1.10_calico:
        requires:
        - build
    - test_1.10_calico_kdd:
        requires:
        - build
    - test_1.10_weave:
        requires:
        - build
    - test_1.10_kube_router:
        requires:
        - build
    - test_1.10_ptp:
        requires:
        - build
    - test_1.11:
        requires:
        - build
    - test_1.11_flannel:
        requires:
        - build
    - test_1.11_calico:
        requires:
        - build
    - test_1.11_calico_kdd:
        requires:
        - build
    - test_1.11_weave:
        requires:
        - build
    - test_1.11_kube_router:
        requires:
        - build
    - test_1.11_ptp:
        requires:
        - build
    - test_1.9_ipv6_only_nat64:
        requires:
        - build
    - test_1.10_ipv6_only_nat64:
        requires:
        - build
    - test_1.11_ipv6_only_nat64:
        requires:
        - build
    - test_1.9_ipv6_only_ext_aaaa:
        requires:
        - build
    - test_1.10_ipv6_only_ext_aaaa:
        requires:
        - build
    - test_1.11_ipv6_only_ext_aaaa:
        requires:
        - build
    - test_src_release:
        requires:
        - build
    - test_src_master:
        requires:
        - build
    - push:
        filters:
          branches:
            only:
            - master
            - stable
        requires:
        - test_1.8
        - test_1.8_flannel
        - test_1.8_calico
        - test_1.8_calico_kdd
        - test_1.8_weave
        - test_1.9
        - test_1.9_flannel
        - test_1.9_calico
        - test_1.9_calico_kdd
        - test_1.9_weave
        - test_1.10
        - test_1.10_flannel
        - test_1.10_calico
        - test_1.10_calico_kdd
        - test_1.10_weave
        - test_1.10_kube_router
        - test_1.10_ptp
        - test_1.11
        - test_1.11_flannel
        - test_1.11_calico
        - test_1.11_calico_kdd
        - test_1.11_weave
        - test_1.11_kube_router
        - test_1.11_ptp
        - test_1.9_ipv6_only_nat64
        - test_1.10_ipv6_only_nat64
        - test_1.11_ipv6_only_nat64
        - test_1.9_ipv6_only_ext_aaaa
        - test_1.10_ipv6_only_ext_aaaa
        - test_1.11_ipv6_only_ext_aaaa
